defaults:
  - _self_
  - /cluster: local
  - /algorithm: ppo
  - environment: cc_cartpole
  - search_space: ppo_cpu_hybrid
  - /experiments: cc_cartpole_ppo
  - override hydra/sweeper: HyperSMAC

hydra:
  sweeper:
    n_trials: 10
    search_space: ${search_space}
    sweeper_kwargs:
      seeds: [0, 1]
      maximize: true
      min_budget: ${environment.n_total_timesteps}
      max_budget: ${environment.n_total_timesteps}
      job_array_size_limit: 1
      optimizer_kwargs:
        smac_facade:
          _target_: smac.facade.blackbox_facade.BlackBoxFacade
          _partial_: true
          overwrite: true
        scenario:
          seed: ${smac_seed}
          n_trials: ${hydra.sweeper.n_trials}
          deterministic: true
          n_workers: 1
          name: ${experiment_name}
          output_directory: results/smac/${algorithm}_${autorl.env_name}_${search_space.name}/${smac_seed}/smac_output
        intensifier:
          _target_: smac.intensifier.Intensifier
          _partial_: true
          max_config_calls: 1   # the number of seeds
          seed: ${smac_seed}
        callbacks:
          - _target_: smac.callback.BudgetExhaustedCallback
            total_resource_budget: 20
            cumulative_cost_tracker: [0]
  run:
    dir: results/smac/${algorithm}_${autorl.env_name}_${search_space.name}/${smac_seed}/${seed}
  sweep:
    dir: results/smac/${algorithm}_${autorl.env_name}_${search_space.name}/${smac_seed}/${seed}
  job:
    chdir: true

experiment_name: ""
load_checkpoint: ""
smac_seed: 0
jax_enable_x64: false
seed: 42

autorl:
  seed: ${seed}
  env_framework: ${environment.framework}
  env_name: ${environment.name}
  env_kwargs: ${environment.kwargs}
  eval_env_kwargs: ${environment.eval_kwargs}
  n_envs: ${environment.n_envs}
  algorithm: ${algorithm}
  cnn_policy: ${environment.cnn_policy}
  nas_config: ${nas_config}
  n_total_timesteps: ${environment.n_total_timesteps}
  checkpoint: []
  checkpoint_name: "default_checkpoint"
  checkpoint_dir: "/tmp"
  state_features: []
  objectives: ["reward_mean", "runtime"]
  optimize_objectives: "upper"
  n_steps: 10
  n_eval_steps: 1
  n_eval_episodes: 128
